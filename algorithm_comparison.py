#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å››ç®—æ³•æ¯”è¾ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. å¯¹æ¯”ç»´çº³æ»¤æ³¢ã€Richardson-Lucyã€U-Netå’ŒMRAS-Netçš„æ€§èƒ½
2. ç”Ÿæˆå®šé‡è¯„ä¼°æŒ‡æ ‡
3. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å’ŒæŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python algorithm_comparison.py
"""

import os
import sys
import numpy as np
import cv2
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import h5py
import json
import time
import psutil
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import ndimage
from scipy.signal import wiener
from skimage import restoration
from skimage.metrics import peak_signal_noise_ratio, structural_similarity, mean_squared_error
from tqdm import tqdm
import yaml
from pathlib import Path

# æ·»åŠ toolsç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'tools'))

from unet_model import UNet
from enhanced_mra_net_model import EnhancedMRANet
from logger import get_logger
from report_generator import PerformanceReportGenerator

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class TestDataset(Dataset):
    """æµ‹è¯•æ•°æ®é›†ç±»"""
    
    def __init__(self, dataset_path: str):
        with h5py.File(dataset_path, 'r') as f:
            self.clean_images = f['clean_images'][:]
            self.blurred_images = f['blurred_images'][:]
            if 'psfs' in f:
                try:
                    self.psfs = f['psfs'][:]
                except (TypeError, KeyError):
                    # å¦‚æœPSFæ•°æ®æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸å­˜åœ¨ï¼Œè®¾ä¸ºNone
                    self.psfs = None
            else:
                self.psfs = None
    
    def __len__(self):
        return len(self.clean_images)
    
    def __getitem__(self, idx):
        clean = self.clean_images[idx].astype(np.float32)
        blurred = self.blurred_images[idx].astype(np.float32)
        
        if self.psfs is not None:
            psf = self.psfs[idx].astype(np.float32)
            return clean, blurred, psf
        else:
            return clean, blurred

class AlgorithmComparison:
    """ç®—æ³•æ¯”è¾ƒä¸»ç±»"""
    
    def _load_config(self) -> Dict:
        """åŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except FileNotFoundError:
            self.logger.warning(f"é…ç½®æ–‡ä»¶ {self.config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                'random_seed': 42,
                'comparison': {
                    'min_samples': 10,
                    'algorithms': {
                        'wiener': {'enabled': True},
                        'richardson_lucy': {'enabled': True, 'iterations': 30},
                        'unet': {'enabled': True},
                        'mra_net': {'enabled': True}
                    }
                }
            }
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def __init__(self, output_dir: str = None, config_path: str = "config/unified_experiment_config.yaml"):
        """
        åˆå§‹åŒ–ç®—æ³•æ¯”è¾ƒå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºoutputs/comparison_YYYYMMDD_HHMMSS
            config_path: ç»Ÿä¸€é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½ç»Ÿä¸€é…ç½®
        self.config_path = config_path
        self.config = self._load_config()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œåˆ™ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„é»˜è®¤ç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"outputs/comparison_{timestamp}"
        self.output_dir = output_dir
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger = get_logger(__name__)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "charts"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "results"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "reports"), exist_ok=True)
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        if 'random_seed' in self.config:
            torch.manual_seed(self.config['random_seed'])
            np.random.seed(self.config['random_seed'])
        
        self.logger.info(f"ç®—æ³•æ¯”è¾ƒå·¥å…·åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        # ç¼“å­˜æ¨¡å‹é¿å…é‡å¤åŠ è½½
        self._mranet_model = None
        self._mranet_loaded_path = None
        # è¯„æµ‹å¢å¼ºé»˜è®¤å‚æ•°ï¼ˆå¯ç”±CLIè¦†ç›–ï¼‰
        self.tta_tile_size = 256
        self.tta_overlap = 32
        self.tta_scales = [1.0]
    
    def wiener_filter(self, blurred_image: np.ndarray, psf: np.ndarray = None, noise_var: float = None) -> np.ndarray:
        """ç»´çº³æ»¤æ³¢ç®—æ³•"""
        if noise_var is None:
            noise_var = self._estimate_noise_variance(blurred_image)
        
        # ä½¿ç”¨scipyçš„ç»´çº³æ»¤æ³¢
        if psf is not None:
            # å¦‚æœæœ‰PSFï¼Œä½¿ç”¨åå·ç§¯
            restored = restoration.wiener(blurred_image, psf, balance=noise_var)
        else:
            # å¦åˆ™ä½¿ç”¨ç®€å•çš„ç»´çº³æ»¤æ³¢
            restored = wiener(blurred_image, noise=noise_var)
        
        return np.clip(restored, 0, 1)
    
    def richardson_lucy(self, blurred_image: np.ndarray, psf: np.ndarray = None, iterations: int = 30) -> np.ndarray:
        """Richardson-Lucyç®—æ³•"""
        if psf is None:
            # å¦‚æœæ²¡æœ‰PSFï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡ç³Šæ ¸
            psf = np.ones((5, 5)) / 25
        
        # ä½¿ç”¨skimageçš„Richardson-Lucyç®—æ³•
        restored = restoration.richardson_lucy(blurred_image, psf, num_iter=iterations)
        return np.clip(restored, 0, 1)
    
    def _estimate_noise_variance(self, image: np.ndarray) -> float:
        """ä¼°è®¡å›¾åƒå™ªå£°æ–¹å·®"""
        # ä½¿ç”¨Laplacianç®—å­ä¼°è®¡å™ªå£°
        laplacian = cv2.Laplacian(image.astype(np.float32), cv2.CV_32F)
        noise_var = np.var(laplacian) * 0.5
        return max(noise_var, 1e-6)
    
    def _test_unet(self, test_loader: DataLoader, model_path: str) -> List[np.ndarray]:
        """æµ‹è¯•U-Netæ¨¡å‹"""
        # åŠ è½½æ¨¡å‹
        model = UNet(n_channels=1, n_classes=1, bilinear=False)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(self.device)
        model.eval()
        
        results = []
        with torch.no_grad():
            for data in test_loader:
                if len(data) == 3:
                    clean, blurred, psf = data
                else:
                    clean, blurred = data
                
                # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶è½¬æ¢ä¸ºtensor
                if isinstance(blurred, torch.Tensor):
                    blurred_tensor = blurred.float().unsqueeze(0).to(self.device)
                    if blurred_tensor.dim() == 3:  # [H, W] -> [1, 1, H, W]
                        blurred_tensor = blurred_tensor.unsqueeze(0)
                else:
                    blurred_tensor = torch.from_numpy(blurred).float().unsqueeze(0).unsqueeze(0).to(self.device)
                
                # é¢„æµ‹ï¼ˆè¯„æµ‹é˜¶æ®µå…³é—­AMPï¼Œå‡å°‘é‡åŒ–è¯¯å·®ï¼‰
                with torch.no_grad():
                    outputs = model(blurred_tensor)
                
                # è½¬æ¢å›numpy
                restored = outputs.squeeze().cpu().numpy()
                if restored.ndim == 3:  # batch dimension
                    restored = restored[0]
                
                results.append(np.clip(restored, 0, 1))
        
        return results
    
    def _test_mra_net(self, test_loader: DataLoader, model_path: str) -> List[np.ndarray]:
        """æµ‹è¯•MRA-Netæ¨¡å‹ï¼ˆTTA+æ»‘çª—é‡å æ¨ç†ï¼‰"""
        # ä»ç»Ÿä¸€é…ç½®åŠ è½½æ¨¡å‹å‚æ•°ï¼Œä½¿ç”¨å¢å¼ºé…ç½®
        model_config = self.config.get('model', {}).get('mra_net', {})
        num_stages = model_config.get('num_stages', 12)  # ä½¿ç”¨å¢å¼ºé…ç½®
        hidden_channels = model_config.get('hidden_channels', 128)  # ä½¿ç”¨å¢å¼ºé…ç½®
        
        # ä»…å½“æœªåŠ è½½æˆ–è·¯å¾„å˜æ›´æ—¶åŠ è½½ä¸€æ¬¡
        if self._mranet_model is None or self._mranet_loaded_path != model_path:
            print(f"åŠ è½½MRAS-Netæ¨¡å‹: stages={num_stages}, channels={hidden_channels}")
            model = EnhancedMRANet(num_stages=num_stages, hidden_channels=hidden_channels)
            checkpoint = torch.load(model_path, map_location=self.device)
            try:
                to_load = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint
                model.load_state_dict(to_load, strict=False)
                print("æˆåŠŸåŠ è½½MRAS-Netæ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨strict=Falseï¼‰")
            except Exception as e:
                print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    print("å°è¯•ä½¿ç”¨checkpointä¸­çš„state_dictåŠ è½½")
                    model.load_state_dict(checkpoint['state_dict'], strict=False)
                else:
                    raise RuntimeError("æ— æ³•åŠ è½½æ¨¡å‹å‚æ•°ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„æˆ–æƒé‡æ–‡ä»¶")
            self._mranet_model = model.to(self.device).eval()
            self._mranet_loaded_path = model_path
        model = self._mranet_model
        
        results = []
        with torch.no_grad():
            for data in test_loader:
                if len(data) == 3:
                    clean, blurred, psf = data
                else:
                    clean, blurred = data
                
                # æ£€æŸ¥æ•°æ®ç±»å‹å¹¶è½¬æ¢ä¸ºtensor
                if isinstance(blurred, torch.Tensor):
                    blurred_tensor = blurred.float().unsqueeze(0).to(self.device)
                    if blurred_tensor.dim() == 3:  # [H, W] -> [1, 1, H, W]
                        blurred_tensor = blurred_tensor.unsqueeze(0)
                else:
                    blurred_tensor = torch.from_numpy(blurred).float().unsqueeze(0).unsqueeze(0).to(self.device)
                
                # è‹¥è¾“å…¥ä¸º0-255ï¼Œå½’ä¸€åŒ–åˆ°[0,1]
                if blurred_tensor.max() > 1.5:
                    blurred_tensor = blurred_tensor / 255.0
                
                # ä½¿ç”¨TTA+æ»‘çª—å¢å¼ºé¢„æµ‹
                restored = self._tta_predict(model, blurred_tensor)
                
                results.append(np.clip(restored, 0, 1))
        
        return results
    
    def _tta_predict(self, model, input_tensor: torch.Tensor) -> np.ndarray:
        """TTAæµ‹è¯•æ—¶å¢å¼ºé¢„æµ‹ï¼ˆå¤šå°ºåº¦ + æ»‘çª—é‡å ï¼‰"""
        # å®šä¹‰8ç§å˜æ¢ï¼šåŸå›¾ + 3ç§90åº¦æ—‹è½¬ + 4ç§ç¿»è½¬
        transforms = [
            lambda x: x,  # åŸå›¾
            lambda x: torch.rot90(x, 1, dims=[2, 3]),  # 90åº¦
            lambda x: torch.rot90(x, 2, dims=[2, 3]),  # 180åº¦
            lambda x: torch.rot90(x, 3, dims=[2, 3]),  # 270åº¦
            lambda x: torch.flip(x, dims=[2]),  # æ°´å¹³ç¿»è½¬
            lambda x: torch.flip(x, dims=[3]),  # å‚ç›´ç¿»è½¬
            lambda x: torch.flip(torch.rot90(x, 1, dims=[2, 3]), dims=[2]),  # 90åº¦+æ°´å¹³ç¿»è½¬
            lambda x: torch.flip(torch.rot90(x, 1, dims=[2, 3]), dims=[3]),  # 90åº¦+å‚ç›´ç¿»è½¬
        ]
        
        # å¯¹åº”çš„é€†å˜æ¢
        inverse_transforms = [
            lambda x: x,  # åŸå›¾
            lambda x: torch.rot90(x, 3, dims=[2, 3]),  # 90åº¦çš„é€†
            lambda x: torch.rot90(x, 2, dims=[2, 3]),  # 180åº¦çš„é€†
            lambda x: torch.rot90(x, 1, dims=[2, 3]),  # 270åº¦çš„é€†
            lambda x: torch.flip(x, dims=[2]),  # æ°´å¹³ç¿»è½¬çš„é€†
            lambda x: torch.flip(x, dims=[3]),  # å‚ç›´ç¿»è½¬çš„é€†
            lambda x: torch.rot90(torch.flip(x, dims=[2]), 3, dims=[2, 3]),  # 90åº¦+æ°´å¹³ç¿»è½¬çš„é€†
            lambda x: torch.rot90(torch.flip(x, dims=[3]), 3, dims=[2, 3]),  # 90åº¦+å‚ç›´ç¿»è½¬çš„é€†
        ]
        
        predictions = []
        
        # å¯¹æ¯ç§å˜æ¢è¿›è¡Œé¢„æµ‹ï¼ˆè¯„æµ‹é˜¶æ®µå…³é—­AMPï¼‰
        for scale in (self.tta_scales if hasattr(self, 'tta_scales') and self.tta_scales else [1.0]):
            for transform, inverse_transform in zip(transforms, inverse_transforms):
                # åº”ç”¨å˜æ¢
                transformed_input = transform(input_tensor)
                # æŒ‰æ¯”ä¾‹ç¼©æ”¾
                if abs(scale - 1.0) > 1e-6:
                    h, w = transformed_input.shape[2], transformed_input.shape[3]
                    new_h = max(64, int(h * scale))
                    new_w = max(64, int(w * scale))
                    transformed_input = torch.nn.functional.interpolate(
                        transformed_input, size=(new_h, new_w), mode='bilinear', align_corners=False
                    )
                # é¢„æµ‹ï¼ˆæ»‘çª—é‡å æ¨ç†ï¼‰
                tile = getattr(self, 'tta_tile_size', 256)
                ovl = getattr(self, 'tta_overlap', 32)
                outputs = self._sliding_window_predict(model, transformed_input, tile_size=int(tile), overlap=int(ovl))
                # ç¼©å›åŸå°ºå¯¸
                if abs(scale - 1.0) > 1e-6:
                    outputs = torch.nn.functional.interpolate(
                        outputs, size=(input_tensor.shape[2], input_tensor.shape[3]), mode='bilinear', align_corners=False
                    )
                # åº”ç”¨é€†å˜æ¢
                restored = inverse_transform(outputs)
                predictions.append(restored)
        
        # å¹³å‡æ‰€æœ‰é¢„æµ‹ç»“æœ
        final_prediction = torch.mean(torch.stack(predictions), dim=0)
        
        # è½¬æ¢å›numpy
        restored_np = final_prediction.squeeze().cpu().numpy()
        if restored_np.ndim == 3:  # batch dimension
            restored_np = restored_np[0]
        
        return restored_np

    def _sliding_window_predict(self, model, input_tensor: torch.Tensor, tile_size: int = 256, overlap: int = 32) -> torch.Tensor:
        """æ»‘çª—é‡å æ¨ç†ï¼Œå‡å°‘è¾¹ç•Œä¼ªå½±

        å‚æ•°:
            model: å·²å¤„äºeval()çš„æ¨¡å‹
            input_tensor: [B=1, C=1, H, W]
            tile_size: æ»‘çª—å°ºå¯¸
            overlap: é‡å åƒç´ 
        è¿”å›:
            è¾“å‡ºå¼ é‡ï¼Œå°ºå¯¸ä¸è¾“å…¥ä¸€è‡´
        """
        assert input_tensor.dim() == 4 and input_tensor.size(0) == 1, "æ»‘çª—æ¨ç†ä»…æ”¯æŒbatch=1"

        _, _, H, W = input_tensor.shape
        # é™åˆ¶tileä¸è¶…è¿‡å›¾åƒå°ºå¯¸ï¼Œé™åˆ¶overlapå°äºtile
        effective_tile = max(32, min(tile_size, int(H), int(W)))
        effective_overlap = int(max(0, min(overlap, effective_tile - 1)))
        step = max(1, effective_tile - effective_overlap)
        # ç´¯è®¡è¾“å‡ºä¸æƒé‡ï¼Œç”¨äºé‡å åŒºåŸŸåŠ æƒå¹³å‡
        output_acc = torch.zeros((1, 1, H, W), device=input_tensor.device, dtype=input_tensor.dtype)
        weight_acc = torch.zeros((1, 1, H, W), device=input_tensor.device, dtype=input_tensor.dtype)

        # ç”Ÿæˆå¹³æ»‘æƒé‡çª—å£ï¼ˆäºŒç»´ä½™å¼¦çª—ï¼‰ï¼Œé™ä½è¾¹ç•Œæ‹¼æ¥ç—•è¿¹
        def _cosine_window(sz: int) -> torch.Tensor:
            x = torch.hann_window(sz, device=input_tensor.device, dtype=input_tensor.dtype)
            w2d = torch.ger(x, x)
            return w2d

        window = _cosine_window(effective_tile)
        window = window / (window.max() + 1e-8)
        window = window.unsqueeze(0).unsqueeze(0)  # [1,1,t,t]

        with torch.no_grad():
            for y in range(0, H, step):
                for x in range(0, W, step):
                    y0 = y
                    x0 = x
                    y1 = min(y0 + effective_tile, H)
                    x1 = min(x0 + effective_tile, W)

                    # è°ƒæ•´èµ·ç‚¹ä»¥ä¿è¯patchå¤§å°ä¸ºtile_sizeï¼ˆé å³/ä¸‹æ—¶å›é€€ï¼‰
                    y0 = max(0, y1 - effective_tile)
                    x0 = max(0, x1 - effective_tile)

                    patch = input_tensor[:, :, y0:y1, x0:x1]
                    # è‹¥è¾¹ç¼˜å¯¼è‡´patchå°äºtile_sizeï¼Œè¿›è¡Œpaddingåˆ°tile_size
                    pad_h = effective_tile - patch.shape[2]
                    pad_w = effective_tile - patch.shape[3]
                    if pad_h > 0 or pad_w > 0:
                        # ä½¿ç”¨replicateé¿å…reflectå¯¹å¤§paddingçš„é™åˆ¶
                        patch = torch.nn.functional.pad(patch, (0, pad_w, 0, pad_h), mode='replicate')

                    # æ¨¡å‹æ¨ç†ï¼ˆè¯„æµ‹é˜¶æ®µå…³é—­AMPï¼‰
                    out_patch, _ = model(patch)

                    # è£å‰ªå›åŸå§‹åŒºåŸŸå¤§å°
                    out_patch = out_patch[:, :, : (y1 - y0), : (x1 - x0)]
                    win = window[:, :, : (y1 - y0), : (x1 - x0)]

                    output_acc[:, :, y0:y1, x0:x1] += out_patch * win
                    weight_acc[:, :, y0:y1, x0:x1] += win

        output = output_acc / (weight_acc + 1e-8)
        return output
    
    def calculate_metrics(self, original: np.ndarray, restored: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        # ç¡®ä¿å›¾åƒåœ¨[0,1]èŒƒå›´å†…
        original = np.clip(original, 0, 1)
        restored = np.clip(restored, 0, 1)
        
        # PSNR
        psnr = peak_signal_noise_ratio(original, restored, data_range=1.0)
        
        # SSIM
        ssim = structural_similarity(original, restored, data_range=1.0)
        
        # MSE
        mse = mean_squared_error(original, restored)
        
        # MAE
        mae = np.mean(np.abs(original - restored))
        
        # è¾¹ç¼˜ä¿æŒæŒ‡æ•°
        epi = self.calculate_edge_preservation(original, restored)
        
        return {
            'psnr': float(psnr),
            'ssim': float(ssim),
            'mse': float(mse),
            'mae': float(mae),
            'edge_preservation': float(epi)
        }
    
    def calculate_edge_preservation(self, original: np.ndarray, restored: np.ndarray) -> float:
        """è®¡ç®—è¾¹ç¼˜ä¿æŒæŒ‡æ•°"""
        try:
            # è®¡ç®—æ¢¯åº¦
            grad_orig_x = cv2.Sobel(original.astype(np.float32), cv2.CV_32F, 1, 0, ksize=3)
            grad_orig_y = cv2.Sobel(original.astype(np.float32), cv2.CV_32F, 0, 1, ksize=3)
            grad_rest_x = cv2.Sobel(restored.astype(np.float32), cv2.CV_32F, 1, 0, ksize=3)
            grad_rest_y = cv2.Sobel(restored.astype(np.float32), cv2.CV_32F, 0, 1, ksize=3)
            
            # è®¡ç®—æ¢¯åº¦å¹…å€¼
            grad_orig = np.sqrt(grad_orig_x**2 + grad_orig_y**2)
            grad_rest = np.sqrt(grad_rest_x**2 + grad_rest_y**2)
            
            # è£å‰ªåˆ°ç›¸åŒå°ºå¯¸
            min_h = min(grad_orig.shape[0], grad_rest.shape[0])
            min_w = min(grad_orig.shape[1], grad_rest.shape[1])
            grad_orig = grad_orig[:min_h, :min_w]
            grad_rest = grad_rest[:min_h, :min_w]
            
            # è®¡ç®—è¾¹ç¼˜ä¿æŒæŒ‡æ•°
            numerator = np.sum(grad_orig * grad_rest)
            denominator = np.sum(grad_orig**2)
            
            if denominator > 0:
                epi = numerator / denominator
            else:
                epi = 0.0
            
            return max(0.0, min(1.0, epi))
        
        except Exception as e:
            self.logger.warning(f"è¾¹ç¼˜ä¿æŒæŒ‡æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def measure_performance(self, func, *args, **kwargs) -> Tuple[Any, Dict[str, float]]:
        """æµ‹é‡ç®—æ³•æ€§èƒ½"""
        # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)
        
        # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        performance = {
            'execution_time': end_time - start_time,
            'memory_usage': end_memory - start_memory,
            'peak_memory': end_memory
        }
        
        return result, performance
    
    def run_comparison(self, 
                      test_dataset_path: str,
                      unet_model_path: str = None,
                      mra_net_model_path: str = None,
                      num_samples: int = None) -> Dict[str, Any]:
        """è¿è¡Œç®—æ³•æ¯”è¾ƒ"""
        self.logger.info("å¼€å§‹è¿è¡Œç®—æ³•æ¯”è¾ƒ")
        
        # åŠ è½½æµ‹è¯•æ•°æ®é›†
        test_dataset = TestDataset(test_dataset_path)
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ ·æœ¬
        min_samples = self.config.get('comparison', {}).get('min_samples', 10)
        if num_samples:
            # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ï¼Œä½†ç¡®ä¿ä¸å°‘äºæœ€å°æ ·æœ¬æ•°
            actual_samples = max(min(num_samples, len(test_dataset)), min_samples)
            test_data = [(test_dataset[i]) for i in range(min(actual_samples, len(test_dataset)))]
        else:
            # ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼Œä½†è‡³å°‘è¦æœ‰æœ€å°æ ·æœ¬æ•°
            if len(test_dataset) < min_samples:
                self.logger.warning(f"æµ‹è¯•æ•°æ®é›†æ ·æœ¬æ•°({len(test_dataset)})å°‘äºå»ºè®®çš„æœ€å°æ ·æœ¬æ•°({min_samples})")
            test_data = [test_dataset[i] for i in range(len(test_dataset))]
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        
        # æ ¹æ®é…ç½®ç¡®å®šè¦æµ‹è¯•çš„ç®—æ³•
        algorithms = []
        alg_list = self.config.get('comparison', {}).get('algorithms', ['Wiener', 'Richardson-Lucy', 'Unet', 'MRA-Net'])
        
        # æ£€æŸ¥æ¯ä¸ªç®—æ³•æ˜¯å¦å¯ç”¨
        if 'Wiener' in alg_list:
            algorithms.append('Wiener')
        if 'Richardson-Lucy' in alg_list:
            algorithms.append('Richardson-Lucy')
        if 'Unet' in alg_list and unet_model_path and os.path.exists(unet_model_path):
            algorithms.append('Unet')
        if 'MRA-Net' in alg_list and mra_net_model_path and os.path.exists(mra_net_model_path):
            algorithms.append('MRAS-Net')  # ä½¿ç”¨MRAS-Netä½œä¸ºåç§°ï¼Œä¸è®ºæ–‡ä¸€è‡´
        
        print(f"å°†æµ‹è¯•ä»¥ä¸‹ç®—æ³•: {algorithms}")
        
        # å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬è¿è¡Œæ‰€æœ‰ç®—æ³•
        for i, data in enumerate(tqdm(test_data, desc="Processing samples")):
            if len(data) == 3:
                clean, blurred, psf = data
            else:
                clean, blurred = data
                psf = None
            
            sample_results = {
                'sample_id': i,
                'algorithms': {}
            }
            
            # 1. ç»´çº³æ»¤æ³¢
            restored, perf = self.measure_performance(
                self.wiener_filter, blurred, psf
            )
            metrics = self.calculate_metrics(clean, restored)
            sample_results['algorithms']['Wiener'] = {
                'metrics': metrics,
                'performance': perf
            }
            
            # 2. Richardson-Lucy
            rl_iterations = self.config.get('comparison', {}).get('richardson_lucy_iterations', 30)
            restored, perf = self.measure_performance(
                self.richardson_lucy, blurred, psf, rl_iterations
            )
            metrics = self.calculate_metrics(clean, restored)
            sample_results['algorithms']['Richardson-Lucy'] = {
                'metrics': metrics,
                'performance': perf
            }
            
            # 3. U-Net (å¦‚æœæ¨¡å‹å­˜åœ¨)
            if 'Unet' in algorithms:
                try:
                    single_loader = DataLoader([data], batch_size=1, shuffle=False)
                    restored_list, perf = self.measure_performance(
                        self._test_unet, single_loader, unet_model_path
                    )
                    restored = restored_list[0]
                    metrics = self.calculate_metrics(clean, restored)
                    sample_results['algorithms']['Unet'] = {
                        'metrics': metrics,
                        'performance': perf
                    }
                except Exception as e:
                    self.logger.warning(f"U-Netæµ‹è¯•å¤±è´¥: {e}")
            
            # 4. MRAS-Net (å¦‚æœæ¨¡å‹å­˜åœ¨)
            if 'MRAS-Net' in algorithms:
                try:
                    single_loader = DataLoader([data], batch_size=1, shuffle=False)
                    restored_list, perf = self.measure_performance(
                        self._test_mra_net, single_loader, mra_net_model_path
                    )
                    restored = restored_list[0]
                    metrics = self.calculate_metrics(clean, restored)
                    sample_results['algorithms']['MRAS-Net'] = {
                        'metrics': metrics,
                        'performance': perf
                    }
                except Exception as e:
                    print(f"è­¦å‘Š: MRAS-Netæµ‹è¯•å¤±è´¥: {e}")
            
            all_results.append(sample_results)
        
        # åˆ†æç»“æœ
        comparison_stats = self._analyze_results(all_results)
        
        # ä¿å­˜ç»“æœ
        self._save_results(all_results, comparison_stats)
        
        # ç”Ÿæˆå›¾è¡¨
        self._generate_charts(comparison_stats)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(comparison_stats)
        
        self.logger.info("ç®—æ³•æ¯”è¾ƒå®Œæˆ")
        return comparison_stats
    
    def _analyze_results(self, all_results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ¯”è¾ƒç»“æœ"""
        algorithms = set()
        for result in all_results:
            algorithms.update(result['algorithms'].keys())
        algorithms = list(algorithms)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        stats = {
            'algorithms': algorithms,
            'num_samples': len(all_results),
            'metrics': {},
            'performance': {},
            'summary': {}
        }
        
        # å¯¹æ¯ä¸ªç®—æ³•è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        for alg in algorithms:
            # æ”¶é›†æŒ‡æ ‡
            metrics_data = {
                'psnr': [],
                'ssim': [],
                'mse': [],
                'mae': [],
                'edge_preservation': []
            }
            
            perf_data = {
                'execution_time': [],
                'memory_usage': [],
                'peak_memory': []
            }
            
            for result in all_results:
                if alg in result['algorithms']:
                    alg_result = result['algorithms'][alg]
                    
                    # æ”¶é›†æŒ‡æ ‡æ•°æ®
                    for metric in metrics_data.keys():
                        if metric in alg_result['metrics']:
                            metrics_data[metric].append(alg_result['metrics'][metric])
                    
                    # æ”¶é›†æ€§èƒ½æ•°æ®
                    for perf in perf_data.keys():
                        if perf in alg_result['performance']:
                            perf_data[perf].append(alg_result['performance'][perf])
            
            # è®¡ç®—ç»Ÿè®¡å€¼
            stats['metrics'][alg] = {}
            for metric, values in metrics_data.items():
                if values:
                    # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬è®¡ç®—æœ‰æ„ä¹‰çš„ç»Ÿè®¡é‡
                    min_samples = self.config.get('comparison', {}).get('min_samples', 10)
                    if len(values) >= min_samples:
                        std_val = float(np.std(values, ddof=1))  # ä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®
                    elif len(values) >= 2:
                        std_val = float(np.std(values, ddof=1))  # ä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®ä½†æ ‡è®°ä¸ºä¸å¯é 
                    else:
                        std_val = 0.0  # å•æ ·æœ¬æ—¶æ ‡å‡†å·®ä¸º0
                    
                    stats['metrics'][alg][metric] = {
                        'mean': float(np.mean(values)),
                        'std': std_val,
                        'min': float(np.min(values)),
                        'max': float(np.max(values)),
                        'median': float(np.median(values)),
                        'count': len(values)  # æ·»åŠ æ ·æœ¬æ•°é‡ä¿¡æ¯
                    }

            stats['performance'][alg] = {}
            for perf, values in perf_data.items():
                if values:
                    # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬è®¡ç®—æœ‰æ„ä¹‰çš„ç»Ÿè®¡é‡
                    min_samples = self.config.get('comparison', {}).get('min_samples', 10)
                    if len(values) >= min_samples:
                        std_val = float(np.std(values, ddof=1))  # ä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®
                    elif len(values) >= 2:
                        std_val = float(np.std(values, ddof=1))  # ä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®ä½†æ ‡è®°ä¸ºä¸å¯é 
                    else:
                        std_val = 0.0  # å•æ ·æœ¬æ—¶æ ‡å‡†å·®ä¸º0
                    
                    stats['performance'][alg][perf] = {
                        'mean': float(np.mean(values)),
                        'std': std_val,
                        'min': float(np.min(values)),
                        'max': float(np.max(values)),
                        'median': float(np.median(values)),
                        'count': len(values)  # æ·»åŠ æ ·æœ¬æ•°é‡ä¿¡æ¯
                    }
        
        # ç”Ÿæˆæ’å
        stats['rankings'] = self._calculate_rankings(stats['metrics'])
        
        return stats
    
    def _calculate_rankings(self, metrics: Dict) -> Dict:
        """è®¡ç®—ç®—æ³•æ’å"""
        rankings = {}
        
        for metric in ['psnr', 'ssim', 'edge_preservation']:
            # è¿™äº›æŒ‡æ ‡è¶Šé«˜è¶Šå¥½
            metric_values = []
            for alg in metrics.keys():
                if metric in metrics[alg]:
                    metric_values.append((alg, metrics[alg][metric]['mean']))
            
            metric_values.sort(key=lambda x: x[1], reverse=True)
            rankings[metric] = [alg for alg, _ in metric_values]
        
        for metric in ['mse', 'mae']:
            # è¿™äº›æŒ‡æ ‡è¶Šä½è¶Šå¥½
            metric_values = []
            for alg in metrics.keys():
                if metric in metrics[alg]:
                    metric_values.append((alg, metrics[alg][metric]['mean']))
            
            metric_values.sort(key=lambda x: x[1])
            rankings[metric] = [alg for alg, _ in metric_values]
        
        return rankings
    
    def _save_results(self, all_results: List[Dict], comparison_stats: Dict):
        """ä¿å­˜ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆåŸå§‹æ ¼å¼ï¼‰
        detailed_results_file = os.path.join(self.output_dir, "results", "detailed_comparison_results.json")
        os.makedirs(os.path.dirname(detailed_results_file), exist_ok=True)
        
        with open(detailed_results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'detailed_results': all_results,
                'statistics': comparison_stats,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è½¬æ¢åçš„ç»“æœï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        converted_results = {}
        for result in all_results:
            sample_id = result['sample_id']
            for alg_name, alg_data in result['algorithms'].items():
                alg_key = alg_name.lower().replace('-', '_')
                if alg_key not in converted_results:
                    converted_results[alg_key] = {'metrics': [], 'times': []}
                
                converted_results[alg_key]['metrics'].append(alg_data['metrics'])
                converted_results[alg_key]['times'].append(alg_data['performance'].get('execution_time', 0))
        
        results_file = os.path.join(self.output_dir, "results", "comparison_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(converted_results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_results_file}")
        self.logger.info(f"è½¬æ¢åç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def _generate_charts(self, stats: Dict):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        algorithms = stats['algorithms']
        
        # 1. PSNRæ¯”è¾ƒ
        plt.figure(figsize=(10, 6))
        psnr_means = [stats['metrics'][alg]['psnr']['mean'] for alg in algorithms if 'psnr' in stats['metrics'][alg]]
        psnr_stds = [stats['metrics'][alg]['psnr']['std'] for alg in algorithms if 'psnr' in stats['metrics'][alg]]
        psnr_algs = [alg for alg in algorithms if 'psnr' in stats['metrics'][alg]]
        
        plt.bar(psnr_algs, psnr_means, yerr=psnr_stds, capsize=5)
        plt.title('PSNR Comparison')
        plt.ylabel('PSNR (dB)')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "charts", "psnr_comparison.png"), dpi=300)
        plt.close()
        
        # 2. SSIMæ¯”è¾ƒ
        plt.figure(figsize=(10, 6))
        ssim_means = [stats['metrics'][alg]['ssim']['mean'] for alg in algorithms if 'ssim' in stats['metrics'][alg]]
        ssim_stds = [stats['metrics'][alg]['ssim']['std'] for alg in algorithms if 'ssim' in stats['metrics'][alg]]
        ssim_algs = [alg for alg in algorithms if 'ssim' in stats['metrics'][alg]]
        
        plt.bar(ssim_algs, ssim_means, yerr=ssim_stds, capsize=5)
        plt.title('SSIM Comparison')
        plt.ylabel('SSIM')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "charts", "ssim_comparison.png"), dpi=300)
        plt.close()
        
        # 3. å¤„ç†æ—¶é—´æ¯”è¾ƒ
        plt.figure(figsize=(10, 6))
        time_means = [stats['performance'][alg]['execution_time']['mean'] for alg in algorithms if 'execution_time' in stats['performance'][alg]]
        time_stds = [stats['performance'][alg]['execution_time']['std'] for alg in algorithms if 'execution_time' in stats['performance'][alg]]
        time_algs = [alg for alg in algorithms if 'execution_time' in stats['performance'][alg]]
        
        plt.bar(time_algs, time_means, yerr=time_stds, capsize=5)
        plt.title('Processing Time Comparison')
        plt.ylabel('Time (seconds)')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "charts", "time_comparison.png"), dpi=300)
        plt.close()
        
        # 4. é›·è¾¾å›¾
        self._create_radar_chart(stats)
        
        # 5. çƒ­åŠ›å›¾
        self._create_heatmap(stats)
        
        self.logger.info("å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
    
    def _create_radar_chart(self, stats: Dict):
        """åˆ›å»ºé›·è¾¾å›¾"""
        algorithms = stats['algorithms']
        metrics = ['psnr', 'ssim', 'edge_preservation']
        
        # å½’ä¸€åŒ–æ•°æ®
        normalized_data = {}
        for alg in algorithms:
            normalized_data[alg] = []
            for metric in metrics:
                if metric in stats['metrics'][alg]:
                    value = stats['metrics'][alg][metric]['mean']
                    # ç®€å•å½’ä¸€åŒ–åˆ°0-1
                    if metric == 'psnr':
                        normalized_value = min(value / 40.0, 1.0)  # å‡è®¾40dBä¸ºæ»¡åˆ†
                    else:
                        normalized_value = value  # SSIMå’ŒEPIå·²ç»åœ¨0-1èŒƒå›´
                    normalized_data[alg].append(normalized_value)
                else:
                    normalized_data[alg].append(0)
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
        
        colors = ['red', 'blue', 'green', 'orange']
        for i, alg in enumerate(algorithms):
            values = normalized_data[alg] + normalized_data[alg][:1]  # é—­åˆæ•°æ®
            ax.plot(angles, values, 'o-', linewidth=2, label=alg, color=colors[i % len(colors)])
            ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.set_title('Algorithm Performance Radar Chart')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "charts", "radar_chart.png"), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_heatmap(self, stats: Dict):
        """åˆ›å»ºçƒ­åŠ›å›¾"""
        algorithms = stats['algorithms']
        metrics = ['psnr', 'ssim', 'mse', 'mae', 'edge_preservation']
        
        # å‡†å¤‡æ•°æ®
        data = []
        for alg in algorithms:
            row = []
            for metric in metrics:
                if metric in stats['metrics'][alg]:
                    value = stats['metrics'][alg][metric]['mean']
                    row.append(value)
                else:
                    row.append(0)
            data.append(row)
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(10, 6))
        sns.heatmap(data, 
                   xticklabels=metrics, 
                   yticklabels=algorithms, 
                   annot=True, 
                   fmt='.3f', 
                   cmap='viridis')
        plt.title('Algorithm Performance Heatmap')
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "charts", "metrics_heatmap.png"), dpi=300)
        plt.close()
    
    def _generate_report(self, stats: Dict):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        try:
            # ç”ŸæˆMarkdownæ ¼å¼çš„æ€§èƒ½åˆ†ææŠ¥å‘Š
            report_content = self._create_markdown_report(stats)
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = os.path.join(self.output_dir, "reports", "performance_analysis_report.md")
            os.makedirs(os.path.dirname(report_file), exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            self.logger.info(f"æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        except Exception as e:
            self.logger.warning(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    def _create_markdown_report(self, stats: Dict) -> str:
        """åˆ›å»ºMarkdownæ ¼å¼çš„æ€§èƒ½åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# Algorithm Performance Analysis Report")
        report.append(f"\n**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"\n**Number of test samples:** {stats['num_samples']}")
        report.append(f"\n**Algorithms tested:** {', '.join(stats['algorithms'])}")
        
        # Executive Summary
        report.append("\n## Executive Summary")
        report.append("\nThis report presents a comprehensive comparison of different image deblurring algorithms.")
        
        # Key Findings
        report.append("\n## Key Findings")
        for alg in stats['algorithms']:
            if alg in stats['metrics']:
                metrics = stats['metrics'][alg]
                report.append(f"\n### {alg}")
                if 'psnr' in metrics:
                    report.append(f"- **PSNR:** {metrics['psnr']['mean']:.3f} Â± {metrics['psnr']['std']:.3f} dB")
                if 'ssim' in metrics:
                    report.append(f"- **SSIM:** {metrics['ssim']['mean']:.3f} Â± {metrics['ssim']['std']:.3f}")
                if alg in stats['performance'] and 'execution_time' in stats['performance'][alg]:
                    perf = stats['performance'][alg]['execution_time']
                    report.append(f"- **Processing Time:** {perf['mean']:.3f} Â± {perf['std']:.3f} seconds")
        
        # Detailed Results
        report.append("\n## Detailed Results")
        report.append("\n### Summary Statistics")
        report.append("\n| Algorithm | PSNR (dB) | SSIM | MSE | MAE | Processing Time (s) |")
        report.append("|-----------|-----------|------|-----|-----|---------------------|")
        
        for alg in stats['algorithms']:
            if alg in stats['metrics']:
                metrics = stats['metrics'][alg]
                perf = stats['performance'].get(alg, {})
                
                psnr = f"{metrics.get('psnr', {}).get('mean', 0):.3f}" if 'psnr' in metrics else "N/A"
                ssim = f"{metrics.get('ssim', {}).get('mean', 0):.3f}" if 'ssim' in metrics else "N/A"
                mse = f"{metrics.get('mse', {}).get('mean', 0):.6f}" if 'mse' in metrics else "N/A"
                mae = f"{metrics.get('mae', {}).get('mean', 0):.6f}" if 'mae' in metrics else "N/A"
                time_val = f"{perf.get('execution_time', {}).get('mean', 0):.3f}" if 'execution_time' in perf else "N/A"
                
                report.append(f"| {alg} | {psnr} | {ssim} | {mse} | {mae} | {time_val} |")
        
        # Algorithm Descriptions
        report.append("\n## Algorithm Descriptions")
        
        for alg in stats['algorithms']:
            if alg in stats['metrics']:
                metrics = stats['metrics'][alg]
                report.append(f"\n### {alg}")
                report.append(f"\n**Descriptive Statistics:**")
                
                for metric_name in ['psnr', 'ssim', 'mse', 'mae', 'edge_preservation']:
                    if metric_name in metrics:
                        metric_data = metrics[metric_name]
                        report.append(f"\n- **{metric_name.upper()}:**")
                        report.append(f"  - Mean: {metric_data['mean']:.6f}")
                        report.append(f"  - Std: {metric_data['std']:.6f}")
                        report.append(f"  - Min: {metric_data['min']:.6f}")
                        report.append(f"  - Max: {metric_data['max']:.6f}")
                        report.append(f"  - Median: {metric_data['median']:.6f}")
                        report.append(f"  - Count: {metric_data['count']}")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    print("=" * 60)
    print("MRA-NetV2 å››ç®—æ³•æ¯”è¾ƒå·¥å…·")
    print("=" * 60)

    parser = argparse.ArgumentParser(description="å››ç®—æ³•æ¯”è¾ƒ")
    parser.add_argument("--test-path", dest="test_path", default="outputs/datasets/test_dataset.h5")
    parser.add_argument("--unet-path", dest="unet_path", default="outputs/models/best_unet.pth")
    parser.add_argument("--mranet-path", dest="mranet_path", default="outputs/models/best_mra_net_fast.pth")
    parser.add_argument("--num-samples", dest="num_samples", type=int, default=None)
    parser.add_argument("--no-interactive", dest="no_interactive", action="store_true", help="éäº¤äº’æ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨æä¾›/é»˜è®¤è·¯å¾„")
    # TTA/æ»‘çª—å‚æ•°
    parser.add_argument("--tta-tile-size", dest="tta_tile_size", type=int, default=384, help="æ»‘çª—tileå¤§å°ï¼Œé»˜è®¤384")
    parser.add_argument("--tta-overlap", dest="tta_overlap", type=int, default=64, help="æ»‘çª—é‡å ï¼Œé»˜è®¤64")
    parser.add_argument("--tta-scales", dest="tta_scales", type=str, default="1.0,0.75", help="å¤šå°ºåº¦TTAæ¯”ä¾‹ï¼Œé€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 1.0,0.75")
    args, unknown = parser.parse_known_args()

    # åˆ›å»ºæ¯”è¾ƒå®ä¾‹
    comparison = AlgorithmComparison()
    # æ³¨å…¥è¯„æµ‹å¢å¼ºå‚æ•°
    try:
        tta_scales = [float(s.strip()) for s in (args.tta_scales.split(',') if args.tta_scales else ["1.0"])]
    except Exception:
        tta_scales = [1.0]
    comparison.tta_tile_size = max(64, int(args.tta_tile_size))
    comparison.tta_overlap = max(0, int(args.tta_overlap))
    comparison.tta_scales = [s for s in tta_scales if s > 0]

    # é»˜è®¤è·¯å¾„
    default_test_path = args.test_path
    default_unet_path = args.unet_path
    default_mra_net_path = args.mranet_path

    if args.no_interactive:
        test_dataset_path = default_test_path
        unet_model_path = default_unet_path if os.path.exists(default_unet_path) else None
        mra_net_model_path = default_mra_net_path if os.path.exists(default_mra_net_path) else None
        if not os.path.exists(test_dataset_path):
            print("âŒ æµ‹è¯•æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼")
            return
        num_samples = args.num_samples
    else:
        # äº¤äº’å¼è¾“å…¥
        test_dataset_path = input(f"è¯·è¾“å…¥æµ‹è¯•æ•°æ®é›†è·¯å¾„ (é»˜è®¤: {default_test_path}): ").strip()
        if not test_dataset_path:
            test_dataset_path = default_test_path
            print(f"ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®é›†: {default_test_path}")
        if not os.path.exists(test_dataset_path):
            print("âŒ æµ‹è¯•æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼")
            return

        # U-Net
        if os.path.exists(default_unet_path):
            print(f"âœ… å‘ç°é»˜è®¤U-Netæ¨¡å‹: {default_unet_path}")
            unet_model_path = input(f"è¯·è¾“å…¥U-Netæ¨¡å‹è·¯å¾„ (é»˜è®¤: {default_unet_path}, ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
            if not unet_model_path:
                unet_model_path = default_unet_path
                print(f"ä½¿ç”¨é»˜è®¤U-Netæ¨¡å‹: {default_unet_path}")
        else:
            unet_model_path = input("è¯·è¾“å…¥U-Netæ¨¡å‹è·¯å¾„ (å¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡): ").strip()
        if unet_model_path and not os.path.exists(unet_model_path):
            print("âš ï¸ U-Netæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡U-Netæµ‹è¯•")
            unet_model_path = None

        # MRAS-Net
        if os.path.exists(default_mra_net_path):
            print(f"âœ… å‘ç°é»˜è®¤MRA-Netæ¨¡å‹: {default_mra_net_path}")
            mra_net_model_path = input(f"è¯·è¾“å…¥MRA-Netæ¨¡å‹è·¯å¾„ (é»˜è®¤: {default_mra_net_path}, ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
            if not mra_net_model_path:
                mra_net_model_path = default_mra_net_path
                print(f"ä½¿ç”¨é»˜è®¤MRA-Netæ¨¡å‹: {default_mra_net_path}")
        else:
            mra_net_model_path = input("è¯·è¾“å…¥MRA-Netæ¨¡å‹è·¯å¾„ (å¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡): ").strip()
        if mra_net_model_path and not os.path.exists(mra_net_model_path):
            print("âš ï¸ MRA-Netæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡MRA-Netæµ‹è¯•")
            mra_net_model_path = None

        num_samples_in = input("è¯·è¾“å…¥æµ‹è¯•æ ·æœ¬æ•°é‡ (å¯é€‰ï¼Œç›´æ¥å›è½¦ä½¿ç”¨å…¨éƒ¨): ").strip()
        if num_samples_in:
            try:
                num_samples = int(num_samples_in)
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„æ ·æœ¬æ•°é‡ï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
                num_samples = None
        else:
            num_samples = None
    
    try:
        print("\nğŸš€ å¼€å§‹ç®—æ³•æ¯”è¾ƒ...")
        
        # è¿è¡Œæ¯”è¾ƒ
        results = comparison.run_comparison(
            test_dataset_path=test_dataset_path,
            unet_model_path=unet_model_path,
            mra_net_model_path=mra_net_model_path,
            num_samples=num_samples
        )
        
        print("\nâœ… ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
        print(f"æµ‹è¯•äº† {len(results['algorithms'])} ä¸ªç®—æ³•")
        print(f"å¤„ç†äº† {results['num_samples']} ä¸ªæ ·æœ¬")
        
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        print("\nğŸ“Š ç®€è¦ç»“æœ:")
        for metric in ['psnr', 'ssim']:
            if metric in results['rankings']:
                print(f"{metric.upper()} æ’å: {' > '.join(results['rankings'][metric])}")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {comparison.output_dir}")
        print("   - å›¾è¡¨: charts/")
        print("   - æ•°æ®: results/")
        print("   - æŠ¥å‘Š: reports/")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()